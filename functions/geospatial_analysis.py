#!/usr/bin/env python
# coding: utf-8

"""
Geospatial analysis codes supporting scripts in 'watershed_tools' repository
  for discretizing watersheds based on elevation, vegetation type, and solar radiation

@authors: orig hongli liu, ncar, 2021
          debugged/revised by andy wood, ncar, 2021

Note: in python, there are two types of nodata mask for raster files.
The first is in the format of GDAL. 0 is invalid region, 255 is valid region. 
The second is in the foramt of Numpy masked array. True is invalid region, False is valid region.
When read mask use ff.read_masks(1), it by default returns mask in the first format.
(reference: https://rasterio.readthedocs.io/en/latest/topics/masks.html)
"""

import os, sys, fiona
from fiona.crs import to_string
import numpy as np
import rasterio as rio
from rasterio.warp import calculate_default_transform, reproject, Resampling
import geopandas as gpd
import fiona.crs 
import rasterio.mask
import pandas as pd
from scipy import stats
from tqdm import tqdm
import matplotlib.pyplot as plt
sys.path.append('../')
import functions.ogr2ogr as ogr2ogr

def reproject_raster(inraster, outraster, dest_crs, resampling_method):
    '''inraster: input, raster, the raster to be re-projected.
    outraster: output, raster, path of output raster.
    dest_crs: new epsg code for output equal-area projection (eg 5070)
    resampling_method: input, rasterio.warp.Resampling method. 
    # resampling_method must be one of the followings:
    # Resampling.nearest, Resampling.bilinear, Resampling.cubic, Resampling.cubic_spline, Resampling.lanczos, Resampling.average, Resampling.mode, 
    # Resampling.max (GDAL >= 2.2), Resampling.min (GDAL >= 2.2), Resampling.med (GDAL >= 2.2), Resampling.q1 (GDAL >= 2.2), Resampling.q3 (GDAL >= 2.2)
    # reference: https://www.earthdatascience.org/courses/use-data-open-source-python/intro-raster-data-python/raster-data-processing/reproject-raster/  
    # reference: https://rasterio.readthedocs.io/en/latest/api/rasterio.warp.html'''

    with rio.open(inraster) as src:
        transform, width, height = calculate_default_transform(
            src.crs, dest_crs, src.width, src.height, *src.bounds)
        kwargs = src.meta.copy()
        kwargs.update({
            'crs': dest_crs,
            'transform': transform,
            'width': width,
            'height': height })

        with rio.open(outraster, 'w', **kwargs) as dst:
            for i in range(1, src.count + 1):
                reproject(
                    source = rio.band(src, i),
                    destination = rio.band(dst, i),
                    src_transform = src.transform,
                    src_crs = src.crs,
                    dst_transform = transform,
                    dst_crs = dest_crs,
                    resampling = resampling_method )
    return

def reproject_basin_shapefile(inShapefile, outShapefile, destination_crs):
    driverName = 'ESRI Shapefile'    # can later be upgraded to work with geopackages (eg 'GPKG')
    driver = ogr.GetDriverByName(driverName)

    spatialref = destination_crs.to_string()    # in wkt format
    ogr2ogr.main(["","-f", driverName, "-t_srs", spatialref, outShapefile, inShapefile])   #note: main is expecting sys.argv, w/ 1st arg as scriptname, hence ""
    return
                
def reproject_vector(invector, outvector, new_epsg):
    '''invector: input shapefile to be re-projected.
       outvector: output reprojected shapefile.
       new_epsg: projection code (eg, 5070) '''
    # reference:https://www.earthdatascience.org/courses/use-data-open-source-python/intro-vector-data-python/vector-data-processing/reproject-vector-data-in-python/
    in_gdf = gpd.read_file(invector)            # read vector file
    in_gdf_prj = in_gdf.to_crs(epsg=new_epsg)   # convert projection   
    in_gdf_prj.to_file(outvector)               # save projected geodataframe
    return 
    
def rasterize_gru_vector(invector, infield, gruId_fieldname, gruNo_fieldname, gruNo_field_dtype, refraster, outraster, gruNo_gruId_txt):
    # reference: https://gis.stackexchange.com/questions/151339/rasterize-a-shapefile-with-geopandas-or-fiona-python
    '''
    invector:          input, vector, the vector to be rasterized.
    infield:           input, str, attribute field of input vector that is used as a burn-in value.
    gruId_fieldname:   input, str, gru name field, add to the updated invector.
    gruNo_fieldname:   input, str, field name of the gru number column, e.g.,1,2,3....
    gruNo_field_dtype: input, str, data type of gruNo_field, used to specify dtype of output gru raster.
    refraster:         input, raster, reference raster to get meta. 
    outraster:         output, raster, path of output raster.
    gruNo_gruId_txt:   output, txt, gruNo-gruId correspondence relationship (csv).    '''
    
    # open input vector
    in_gdf = gpd.read_file(invector)  
    in_gdf = in_gdf.sort_values(by=[infield])

    # add a gruName_field to gdf
    in_gdf[gruId_fieldname] = in_gdf[infield]
    in_gdf[gruId_fieldname] = pd.to_numeric(in_gdf[gruId_fieldname], errors='coerce')

    # add a gruNo_field to gdf
    in_gdf[gruNo_fieldname] = np.arange(1,len(in_gdf)+1)
    in_gdf[gruNo_fieldname] = pd.to_numeric(in_gdf[gruNo_fieldname], errors='coerce')

    # save the correspondence between gruNo and gruId
    in_gdf[[gruNo_fieldname, gruId_fieldname]].to_csv(gruNo_gruId_txt, sep=',', index=False)    

    # copy and update the metadata from the input raster for the output
    # avaialble dtypes for rasterio: 'int16', 'int32', 'float32', 'float64' 
    # reference: https://test2.biogeo.ucdavis.edu/rasterio/_modules/rasterio/dtypes.html
    with rio.open(refraster) as src:                 
        ref_mask = src.read_masks(1)
        meta = src.meta.copy()
        nodatavals = src.nodatavals
    meta.update(count=1, dtype=gruNo_field_dtype, compress='lzw')

    with rio.open(outraster, 'w+', **meta) as out:
        out_arr   = out.read(1)

        # burn the features into the raster and write it out
        # this is where we create a generator of geom, value pairs to use in rasterizing
        shapes    = ((geom,value) for geom, value in zip(in_gdf['geometry'], in_gdf[gruNo_fieldname]))  
        # Areas not covered by input geometries are replaced with an optional fill value, which defaults to 0.
        burned    = rio.features.rasterize(shapes=shapes, out=out_arr, fill=nodatavals, transform=out.transform)
        burned_ma = np.ma.masked_array(burned, ref_mask==0)
        out.write(burned_ma,1) 
    
    # save updated gru shapefile
    in_gdf.to_file(invector) 
    return

def rasterize_vector(invector,infield,infield_dtype,refraster,outraster):
    # reference: https://gis.stackexchange.com/questions/151339/rasterize-a-shapefile-with-geopandas-or-fiona-python
    '''
    invector: input, vector, the vector to be rasterized.
    infield: input, string. field of input vector to use as a burn-in value.
    infield_dtype: input, string. data type of infield, used to rasterize invector.
    Note: Avaialble dtypes for rasterio: 'int16', 'int32', 'float32', 'float64'(reference: https://test2.biogeo.ucdavis.edu/rasterio/_modules/rasterio/dtypes.html)
    refraster: input, raster, reference raster to get meta. 
    outraster: output, raster, path of output raster.    '''
    
    # open input vector
    in_gdf = gpd.read_file(invector)    
        
    # copy and update the metadata from the refraster for the output
    with rio.open(refraster) as src:                 
        ref_mask = src.read_masks(1)
        meta = src.meta.copy()
        nodatavals = src.nodatavals
    meta.update(count=1, dtype=infield_dtype, compress='lzw') 
    
    with rio.open(outraster, 'w+', **meta) as out:
        out_arr = out.read(1)
    
        # burn the features into the raster and write it out
        # this is where we create a generator of geom, value pairs to use in rasterizing
        shapes = ((geom,value) for geom, value in zip(in_gdf['geometry'], in_gdf[infield]))  
        # Areas not covered by input geometries are replaced with an fill value.
        # Note that this process may induce pixels missing if the pixels' center is not within the polygon.
        # reference: https://rasterio.readthedocs.io/en/latest/api/rasterio.features.html
        burned = rio.features.rasterize(shapes=shapes, out=out_arr, fill=nodatavals, transform=out.transform)
        burned_ma = np.ma.masked_array(burned, ref_mask==0)
        out.write(burned_ma,1)    
    return

def crop_raster(inraster, invector, outraster):
    '''inraster: input, raster, raster to be cropped.
       invector: input, vector, provide crop extent.
      outraster: output, raster, output raster after crop.'''
    
    # open crop extent
    in_gdf = gpd.read_file(invector) 
    shapes = in_gdf['geometry']    
    
    # crop raster with the invector extent
    with rio.open(inraster) as src:
        if src.crs == in_gdf.crs:     # check projection consistency
            out_arr, out_transform = rio.mask.mask(src, shapes, crop=True)
            out_meta = src.meta.copy()
        else:
            print('Crop failed because input raster and vector are in different projections.')
     
    # update the new cropped meta info
    out_meta.update({"driver":    "GTiff",
                     "height":    out_arr.shape[1],
                     "width":     out_arr.shape[2],
                     "transform": out_transform})
    
    # write cropped raster data
    with rio.open(outraster, 'w', **out_meta) as outf:
        out_arr_ma = np.ma.masked_array(out_arr)
        outf.write(out_arr_ma)            
    return

def calculate_slope_and_aspect(dem_raster,slope_raster,aspect_raster):
    '''dem_raster: input, DEM raster.
    slope_raster: output, slope raster.
    aspect_raster: output, aspect raster.'''
    # reference: https://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-slope-works.htm
    # reference: https://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-analyst-toolbox/how-aspect-works.htm

    # read dem input
    with rio.open(dem_raster) as ff:
        dem  = ff.read(1)
        mask = ff.read_masks(1)
        out_meta = ff.meta.copy()
        nodatavals = ff.nodatavals
        pixelSizeX, pixelSizeY  = ff.res

    # # calculate slope and aspect
    slope = dem.copy()
    aspect = dem.copy()
    (nx,ny) = np.shape(dem)
    # X is latitude, Y is longitude.
    #                             j(ny)
    #     |------------------------->
    #     |            North
    #     |        ------------
    #     |         a | b | c
    #     |        ------------
    #     |  West   d | e | f    East
    #     |        ------------
    #     |         g | h | i
    #  i  |        ------------
    # (nx)↓           South

    for i in range(nx): # latitude (north -> south)
        for j in range(ny): # longitude (west -> east)
            if (dem[i,j] != nodatavals):
                # neighbor index
                isouth = i+1
                inorth = i-1
                jwest = j-1
                jeast = j+1

                if (inorth < 0):
                    inorth = 0
                if (isouth > nx-1):
                    isouth = nx-1
                if (jwest < 0):
                    jwest = 0
                if (jeast > ny-1):
                    jeast = ny-1

                # neighbor elevation            
                dem_a = dem[inorth,jwest]
                dem_b = dem[inorth,j]
                dem_c = dem[inorth,jeast]
                dem_d = dem[i,jwest]
                dem_e = dem[i,j]
                dem_f = dem[i,jeast]
                dem_g = dem[isouth,jwest]
                dem_h = dem[isouth,j]
                dem_i = dem[isouth,jeast]

                # dz/dx and dz/dy
                dzdx = ((dem_c+2*dem_f+dem_i) - (dem_a+2*dem_d+dem_g))/(8*pixelSizeX)
                dzdy = ((dem_g+2*dem_h+dem_i) - (dem_a+2*dem_b+dem_c))/(8*pixelSizeY)

                # slope in rise_run (radian)
                rise_run = (dzdx**2 + dzdy**2)**0.5            
                # slope in degree [0, 90]
                slope[i,j] = np.degrees(np.arctan(rise_run))

                # aspect in degree [-180, 180]
                aspect0 = np.degrees(np.arctan2(dzdy,-dzdx))
                # convert to compass direction values (0-360 degrees)
                if aspect0 < 0:
                    cell = 90.0 - aspect0
                elif aspect0 > 90.0:
                    cell = 360.0 - aspect0 + 90.0
                else:
                    cell = 90.0 - aspect0
                aspect[i,j] = cell
    
    # let aspect = 0 for flat grids (slope=0).
    aspect[slope==0.0] = 0
    
    # save slope and aspect into raster
    with rio.open(slope_raster, 'w', **out_meta) as outf:
        slope_ma = np.ma.masked_array(slope, mask==0)
        outf.write(slope_ma,1)       
    with rio.open(aspect_raster, 'w', **out_meta) as outf:
        aspect_ma = np.ma.masked_array(aspect, mask==0)
        outf.write(aspect_ma,1)  
    return


def resample_raster(inraster,refraster,outraster):
    '''
    inraster: input, raster, rater to be cropped.
    refraster: input, raster, provide reference of array shape.
    outraster: output, raster, output raster after crop.'''
    
    # read refraster to get band size
    with rio.open(refraster) as ff:
        ref_crs = ff.crs
        ref_mask = ff.read_masks(1)
        ref_height, ref_width = ff.height, ff.width
        
    # method 1. use rasterio
    with rio.open(inraster) as ff:
        data  = ff.read(1)
        out_meta = ff.meta.copy()

        # reference: https://rasterio.readthedocs.io/en/stable/topics/resampling.html
        with rio.open(inraster) as ff:
            # resample data to target shape
            data = ff.read(
                out_shape=(int(ref_height),int(ref_width)),
                resampling=rio.enums.Resampling.nearest)

            # scale image transform
            transform = ff.transform * ff.transform.scale(
                (ff.width / data.shape[-1]),
                (ff.height / data.shape[-2]))

    # write resampled raster data
    out_meta.update(transform=transform, driver='GTiff', 
                    height=ref_height, width=ref_width, crs=ref_crs)

    with rio.open(outraster, 'w', **out_meta) as outf:
        out_arr_ma = np.ma.masked_array(data, ref_mask==0)
        outf.write(out_arr_ma) 

    # method 2. use gdal 
    # reference: https://gis.stackexchange.com/questions/271226/up-sampling-increasing-resolution-raster-image-using-gdal
    return

def resample_raster_scale(inraster,scale_factor,outraster):
    # This function is used when the scale_factor is provided by users. 
    # The function has not been used in this project yet, but it's put here in case it can be used later.
    with rio.open(inraster) as dataset:

        # resample data to target shape
        data = dataset.read(
            out_shape=(dataset.count,
                       int(dataset.height * scale_factor),
                       int(dataset.width * scale_factor)),
            resampling=Resampling.bilinear)

        # scale image transform
        transform = dataset.transform * dataset.transform.scale(
            (dataset.width / data.shape[-1]),
            (dataset.height / data.shape[-2]))

        # write resampled raster data
        out_meta = dataset.meta.copy()
        out_meta.update(transform=transform, driver='GTiff', 
                        height=int(dataset.height * scale_factor),
                        width=int(dataset.width * scale_factor))

        with rio.open(outraster, 'w', **out_meta) as outf:
            out_arr_ma = np.ma.masked_array(data, data==dataset.nodatavals)
            outf.write(out_arr_ma) 
        return
    
    
def classify_raster(inraster, bound_raster, classif_trigger, bins, class_outraster, value_outraster):
    '''
    bound_raster: input, raster, boundary as mask to classify inraster per boundary unit (eg, GRU raster).
    inraster: input, raster, continuous raster file.
    classif_trigger: input, numeric, inraster value difference to trigger classification. 
        - When the raster value difference is beyond classif_trigger value, implement classification. 
        - Otherwise, do not implement the classification.
    bins: input, Three types:
        - If bins is an int, it defines the number of equal-width bins in the given range. 
        - If bins is a sequence, it defines a monotonically increasing array of bin edges, 
        including the rightmost edge, allowing for non-uniform bin widths.
        - If bins is a string, it defines the method used to calculate the bin edge sequence.
    class_outraster: output, raster, output raster of class.
    value_outraster: output, raster, output raster of mean value per class. '''
    """
    # read inraster data
    with rio.open(inraster) as ff:
        data  = ff.read(1)
        data_mask = ff.read_masks(1)
        out_meta = ff.meta.copy()

    # read bound_raster and identify unique bounds
    with rio.open(bound_raster) as ff:
        bounds  = ff.read(1)
        bounds_mask = ff.read_masks(1)
    unique_bounds = np.unique(bounds[bounds_mask!=0])

    # define array in the same shape of data, and specify dtype!
    data_class = data.copy()
    data_value = data.copy()
    """
    # Read inraster data and create a mask for NoData values
    with rio.open(inraster) as ff:
        data = ff.read(1)
        nodata_mask = data == -9999  # Mask where NoData values are -9999
        data_mask = ff.read_masks(1)
        valid_data_mask = (data_mask != 0) & ~nodata_mask  # Combine masks to exclude NoData
        out_meta = ff.meta.copy()
        masked_data = data * valid_data_mask


    # Read bound_raster and identify unique bounds excluding NoData values
    with rio.open(bound_raster) as ff:
        bounds = ff.read(1)
        bounds_mask = ff.read_masks(1)
    unique_bounds = np.unique(bounds[(bounds_mask != 0) & ~nodata_mask])
    print(f'unique bound{unique_bounds}')
    # Initialize classification and value arrays
    data_class = np.full(data.shape, -9999, dtype='int32')  # Use NoData value for initialization
    data_value = np.full(data.shape, -9999, dtype='float64')  # Use NoData value for initialization


    # reclassify raster 
    # (1) if 'bins' is a string
    if isinstance(bins, str):
        bin_name = bins

        if bin_name == 'median':
            # loop through bounds
            for bound in unique_bounds:
                smask = bounds == bound            
                # Bin the raster data based on the data median
                smin,smedian,smax=np.min(data[smask]), np.median(data[smask]), np.max(data[smask])
                # use smedian to do classification in two conditions: 
                # when not given classif_trigger;
                # when given classif_trigger, and smax-smin is larger than classif_trigger. 
                print('gru %d, raster value diff %f.'%(bound,smax-smin))
                if (classif_trigger is None) or \
                ((classif_trigger is not None) and ((smax-smin)>=classif_trigger)):
                    bins = [smin,smedian,smax]
                else:
                    bins = [smin,smax]
                (hist,bin_edges) = np.histogram(data[smask],bins=bins)            
                #Place the mean elev and elev band
                for ibin in np.arange(len(bin_edges)-1):
                    if ibin!= (len(bin_edges)-1-1):
                        smask = (bounds == bound) & (data >= bin_edges[ibin]) & (data < bin_edges[ibin+1])
                    else: 
                        # All but the last (righthand-most) bin is half-open.  
                        # reference: https://github.com/numpy/numpy/blob/v1.19.0/numpy/lib/histograms.py#L677-L928
                        smask = (bounds == bound) & (data >= bin_edges[ibin]) & (data <= bin_edges[ibin+1])
                    data_class[smask] = ibin+1  # ele_band starts from one, not zero.
                    data_value[smask] = np.mean(data[smask])     

    # (2) if 'bins' is an integer or a sequence
    elif (np.ndim(bins) == 0) or (np.ndim(bins) == 1): 
        bin_settings = bins
        # loop through bounds
        for bound in unique_bounds:
            smask = bounds == bound
            smin,smedian,smax=np.min(data[smask]), np.median(data[smask]), np.max(data[smask])
            print('gru %d, raster value diff %f.'%(bound,smax-smin))
            if (classif_trigger is not None) and ((smax-smin)<=classif_trigger):
                bins = [smin,smax]
            else:
                bins = bin_settings
            # Bin the raster data based on bins
            (hist,bin_edges) = np.histogram(data[smask],bins=bins)
            print(f'bin edges: {bin_edges}')
            #Place the mean elev and elev band
            for ibin in np.arange(len(bin_edges)-1):
                if ibin!= (len(bin_edges)-1-1):
                    smask = (bounds == bound) & (data >= bin_edges[ibin]) & (data < bin_edges[ibin+1])
                else: 
                    smask = (bounds == bound) & (data >= bin_edges[ibin]) & (data <= bin_edges[ibin+1])
                data_class[smask] = ibin+1  # ele_band starts from one, not zero.
                data_value[smask] = np.mean(data[smask])     

    # save data_class_ma and data_value_ma into rasters
    # update data type of meta and save raster
    data_class = data_class.astype('int32') 
    out_meta.update(count=1, dtype='int32', compress='lzw')
    with rio.open(class_outraster, 'w', **out_meta) as outf:
        data_class_ma = np.ma.masked_array(data_class,data_mask==0)
        outf.write(data_class_ma, 1)    

    # update data type of meta and save raster
    data_value = data_value.astype('float64') 
    out_meta.update(count=1, dtype='float64', compress='lzw')
    with rio.open(value_outraster, 'w', **out_meta) as outf:
        data_value_ma = np.ma.masked_array(data_value,data_mask==0) 
        outf.write(data_value_ma, 1)
    return

def classify_landcover(lc_raster, lc_class_raster):
    '''
    lc_raster: input, raster, landcover raster file.
    lc_class_raster: output, raster, raster class file.'''
    
    # read landcover data
    with rio.open(lc_raster) as ff:
        lc  = ff.read(1)
        lc_mask = ff.read_masks(1)
        out_meta = ff.meta.copy()

    # define array and specify dtype!
    lc_class = lc.copy()

    # divide landcover classes into canopy and non-canopy based on the IGBP classifications
    # lc_class 1: canopy 2: non-canopy
    ## Canopy class includes: 1 Evergreen needleleaf forests, 2 Evergreen broadleaf forests, 
    ## 3 Deciduous needleleaf forests, 4 Deciduous broadleaf forests, 5 Mixed forests, 6 Closed shrublands, 8 Woody savannas.  
    lc_class[(lc>0) & (lc<=7)]   = 1 # canopy
    lc_class[((lc>7) & (lc!=255)) | (lc==0) ] = 2 # non-canopy

    # convert class dtype from float to int
    # avaialble dtypes for rasterio: 'int16', 'int32', 'float32', 'float64' 
    # reference: https://test2.biogeo.ucdavis.edu/rasterio/_modules/rasterio/dtypes.html
    lc_class = lc_class.astype('int32') 
    out_meta.update(count=1, dtype='int32', compress='lzw')
    
    # save data_class to raster
    with rio.open(lc_class_raster, 'w', **out_meta) as outf:
        lc_class_ma = np.ma.masked_array(lc_class,lc_mask==0)
        outf.write(lc_class_ma, 1)    
    return

def classify_aspect(aspect_raster, class_num, class_outraster):
    '''
    inraster: input, raster, continuous raster file.
    class_num: input, int, it defines the number of desired aspect classes. 
    class_outraster: output, raster, output raster of class.
    value_outraster: output, raster, output raster of mean value per class. '''
    
    # read aspect_raster
    with rio.open(aspect_raster) as ff:
        data  = ff.read(1)
        mask = ff.read_masks(1)
        nodatavals = ff.nodatavals
        out_meta = ff.meta.copy()
    data_ma = np.ma.masked_array(data, mask==0)

    # define array in the same shape of data, and specify dtype!
    data_class = data.copy()

    # reclassify aspect
    # reference: https://www.neonscience.org/resources/learning-hub/tutorials/classify-raster-thresholds-py
    if class_num == 4:
        data_class[np.where((data_ma==0))] = 0                    # flat
        data_class[np.where((data_ma>0) & (data_ma<=45))] = 1     # north
        data_class[np.where((data_ma>45) & (data_ma<=135))] = 2   # east
        data_class[np.where((data_ma>135) & (data_ma<=225))] = 3  # south
        data_class[np.where((data_ma>225) & (data_ma<=315))] = 4  # west
        data_class[np.where((data_ma>315) & (data_ma<=360))] = 1  # north
    elif class_num == 8:
        data_class[np.where((data_ma==0))] = 0                       # flat
        data_class[np.where((data_ma>0) & (data_ma<=22.5))] = 1      # north
        data_class[np.where((data_ma>22.5) & (data_ma<=67.5))] = 2   # northeast
        data_class[np.where((data_ma>67.5) & (data_ma<=112.5))] = 3  # east
        data_class[np.where((data_ma>112.5) & (data_ma<=157.5))] = 4 # southeast
        data_class[np.where((data_ma>157.5) & (data_ma<=202.5))] = 5 # south
        data_class[np.where((data_ma>202.5) & (data_ma<=247.5))] = 6 # southwest
        data_class[np.where((data_ma>247.5) & (data_ma<=292.5))] = 7 # west
        data_class[np.where((data_ma>292.5) & (data_ma<=337.5))] = 8 # northwest
        data_class[np.where((data_ma>337.5) & (data_ma<=360))] = 1   # north
    
    # convert class dtype from float to int
    data_class = data_class.astype('int32') 
    out_meta.update(count=1, dtype='int32', compress='lzw')
    
    # save data_class to raster
    with rio.open(class_outraster, 'w', **out_meta) as outf:
        data_class_ma = np.ma.masked_array(data_class,mask=0)
        outf.write(data_class_ma, 1)    
    return

def polygonize_raster(inraster, outvector, attrb_field, attrb_field_dtype):
    '''
    inraster: input, raster, source to convert to vector.
    outvector: output, vector, output vector.
    attrb_field: input, str, attribute field name of vector attribtue table to save raster values.
    attrb_field_dtype: input, str, type of the output attribute field. For example, int'''
    # reference: https://programtalk.com/vs2/?source=python/7910/rasterio/examples/rasterio_polygonize.py
    
    driver='ESRI Shapefile'     
    with rio.Env():
        
        # read raster
        with rio.open(inraster) as src:
            image = src.read(1)
            mask = src.read_masks(1)
            
        # extract shapes of raster features
        results = (
            {'properties': {attrb_field: v}, 'geometry': s}
            for i, (s, v) in enumerate(rio.features.shapes(image, mask=mask, transform=src.transform)))
        
        # write GeoJSON style geometry dict to shapefile
        with fiona.open(
                outvector, 'w',
                driver=driver,
                crs=fiona.crs.to_string(src.crs),
                schema={'properties': [(attrb_field, attrb_field_dtype)],
                        'geometry': 'Polygon'}) as dst:
            dst.writerecords(results)      
    return

# Create HRU by overlaying a list of provided input rasters.
def define_hru(raster_list, fieldname_list, gru_raster, gruNo_hucId_txt, gruNo_field, gruName_field,
               outraster, outvector, hruNo_field, hruNo_field_dtype, hruName_field):
    '''
    raster_list: input, list, a list raster inputs that are used to define HRU.
    fieldname_list: input, str, a list of field names corresponding to raster_list.
    gru_raster: input, raster, gru input raster.
    gruNo_field: input, string, field name of the gru number column, e.g.,1,2,3... 
    gruNo_hucId_txt: input, text, gruNo-HUC correspondence txt file.
    outraster: output, raster, HRU raster with integer value.
    outvector: output, raster, HRU vector with HRU_full and HRU_int fields. '''
    
    # remove output files if exist
    for file in [outraster,outvector]:
        if os.path.exists(file): 
            os.remove(file)
        
    # --- PART 1. rasterize HRU via raster overlay (array concatenation) ---
    raster_str_max_len_list = [] # recrod the max str lenght of input class str
    for iraster, raster in enumerate(raster_list):
        print('raster %d: %s'%(iraster,raster))
        # read raster
        with rio.open(raster) as ff:
            raster_value = ff.read(1)
            raster_mask  = ff.read_masks(1)

        # convert raster value to str, and format str to the same length (i.e., the max str length) 
        raster_str = raster_value.astype(str)
        max_len    = max(map(len,raster_str[raster_mask!=0]))
        raster_str_max_len_list.append(max_len)
        raster_str_fmt = np.char.zfill(raster_str,max_len)

        # concatenate element-wise two arrays to generate HRU
        if iraster == 0:
            hru_str_fmt = raster_str_fmt
            hru_mask    = (raster_mask!=0)
        else:
            hru_str_fmt = np.char.add(hru_str_fmt, raster_str_fmt)
            hru_mask = (hru_mask & raster_mask)

    # assign the unique integer to unique HRU (HRU_str_fmt)
    # note: the defined numpy array dtype needs to be consistent with the available rasterio dytpes (e.g., int32,float32,float64).
    # note: rasterio dytpes doesn't have int64.
    hru_int = np.zeros(np.shape(hru_str_fmt), dtype=np.int32)
    unique_hrus_str = np.unique(hru_str_fmt[hru_mask!=0])
    for ihru, hru in enumerate(unique_hrus_str):
        hru_mask_temp = hru_str_fmt == hru
        hru_int[hru_mask_temp] = int(ihru)+1
    hru_int_ma = np.ma.masked_array(hru_int,hru_mask==0)

    # save hru_int_ma into raster based on gru_raster 
    with rio.open(gru_raster) as src:
        dst_meta = src.meta.copy()
    with rio.open(outraster, 'w', **dst_meta) as dst:
        dst.write(hru_int_ma, 1)

    # --- PART 2. polgonize HRU and add attributes ---
    # polygonize hru rater
    polygonize_raster(outraster, outvector, hruNo_field, hruNo_field_dtype)

    # dissolve and exclude HRU=0 blank area
    hru_gpd = gpd.read_file(outvector)
    # use buffer(0) to fix self-intersection issue
    hru_gpd['geometry'] = hru_gpd.geometry.buffer(0)
    
    hru_gpd_disv = hru_gpd.dissolve(by=hruNo_field)
    hru_gpd_disv = hru_gpd_disv.reset_index()
    hru_gpd_disv = hru_gpd_disv.loc[hru_gpd_disv.loc[:,hruNo_field] > 0]
    hru_gpd_disv = hru_gpd_disv.reset_index()

    # add other fields: HRU and detailed input raster classes
    # Note: since we want to name HRU as HUC12+hru# (xxxx01, xxxx02), 
    # we need to find out the string location of gru among the compelte hru str.
    gru_index = raster_list.index(gru_raster)
    gru_str_start = sum(raster_str_max_len_list[0:gru_index])
    gru_str_len = raster_str_max_len_list[gru_index]

    # read gruNo-HUC12 corresponding text file to get gruNo <-> hucID.
    corr_df = pd.read_csv(gruNo_hucId_txt, sep=",")

    # loop through HRUs to add attributes
    for irow, row in hru_gpd_disv.iterrows():

        # (a) add new field columns, define other needed variables.
        if irow == 0:
            hru_gpd_disv[hruName_field] = ""    # simplified HRU name, e.g., hruNo + HRU#.
            hru_gpd_disv[gruName_field] = ""    # HUC int
            for jfield, field in enumerate(fieldname_list): # other fields
                hru_gpd_disv[field] = ""          
            hru_num_per_gru = 0           # count the number of HRUs per gru
            gruNo_before    = ''             # gruNo before this irow to update hru_num_per_gru
        else:
            gruNo_before = gruNo_current

        # (b) identify current gru ID (gruNo_current: 1,2,3...)
        hru_str       = unique_hrus_str[row[hruNo_field]-1]
        gruNo_current = hru_str[gru_str_start:gru_str_start+gru_str_len] 

        # (c) update the number of HRUs within gruNo_current (hru_num_per_gru: 1,2)
        if gruNo_current == gruNo_before:  # if the current gruNo is the same as before, cumulate hru num.
            hru_num_per_gru = hru_num_per_gru+1
        else: # otherwise, reset hru num as zero.
            hru_num_per_gru = 0
        import ast
        # (d) fill hruName_field = gruNo + hru_count in gruNo_current, and HUC12 field.   
        #gruName = corr_df[corr_df[gruNo_field]==ast.literal_eval(gruNo_current)].reset_index().loc[0,gruName_field]
        gruName = gruNo_current
        hru_gpd_disv.loc[irow,hruName_field] = str(gruName) + str(hru_num_per_gru+1).zfill(2) # two-digit HRU#
        hru_gpd_disv.loc[irow,gruName_field] = str(gruName) 

        # (e) fill other fields for class checks 
        for jfield, field in enumerate(fieldname_list):
            field_str_start = sum(raster_str_max_len_list[0:jfield])
            field_str_len   = raster_str_max_len_list[jfield]
            hru_gpd_disv.at[irow,field] = int(float(hru_str[field_str_start:field_str_start+field_str_len]))  
    
    # change dtypes of name and class fields to be int64
    hru_gpd_disv[hruName_field] = pd.to_numeric(hru_gpd_disv[hruName_field], errors='coerce')
    hru_gpd_disv[gruName_field] = pd.to_numeric(hru_gpd_disv[gruName_field], errors='coerce')
    for field in fieldname_list:   
        hru_gpd_disv[field] = pd.to_numeric(hru_gpd_disv[field], errors='coerce')
        
    # save gpd to shapefile
    hru_gpd_disv.to_file(outvector, index=False)
    return

# Eliminate small HRUs by merging with the most dominant HRU within the same GRU.
def eliminate_small_hrus_dominant(hru_vector, hru_area_thld, gruNo_field, gruName_field, 
                                  hruNo_field, hruNo_field_dtype, hruName_field, hruArea_field, 
                                  fieldname_list, refraster, hru_vector_disv, hru_raster_disv):
    '''
    hru_vector:        input, HRU shapefile.
    hru_area_thld:     input, number, small HRU area threthold below which the HRU will be merged with another HRU.
    gruNo_field:       input, string, field name of the gru number column, e.g.,1,2,3... 
    gruName_field:     input, string, field name of gru name, e.g., 100800120101. 
    hruNo_field:       input, string, field name of the hru number column, e.g.,1,2,3...
    hruNo_field_dtype: input, string, data type of hruNo_field, used to save hru raster.
    hruName_field:     input, string, field name of the hru name column, e.g., 10080012010101, 100800120102. 
    hruArea_field:     input, string, field name of the HRU area, used in small HRU elimination.
    fieldname_list:    input, string list, a list of filed names used in hru generation.
    refraster:         input, raster, a reference raster to get meta when rasterizing outvector.
    outraster:         output, raster, HRU raster with integer value.
    outvector:         output, raster, HRU vector with HRU_full and HRU_int fields. '''
    
    in_gpd = gpd.read_file(hru_vector)
    in_gpd_disv = in_gpd.copy()

    # PART 1. eliminate small HRUs based on HRU area threshold
    grus = np.unique(in_gpd_disv[gruNo_field].values)
    pbar = tqdm(total=len(grus))
    for gru in grus:
        in_gpd_disv[in_gpd_disv[gruNo_field]==gru][hruArea_field]
        max_index = in_gpd_disv[in_gpd_disv[gruNo_field]==gru][hruArea_field].argmax()
        flt1      = (in_gpd_disv[gruNo_field]==gru)                 # filter 1: gru
        flt2      = (in_gpd_disv[hruArea_field]<=hru_area_thld)     # filter 2: HRU area    
        flt       = (flt1 & flt2)
        # change attributes to the most dominant ones
        for field in fieldname_list:
            in_gpd_disv.at[flt,field]     = in_gpd_disv.loc[max_index,field]
        in_gpd_disv.at[flt,hruName_field] = in_gpd_disv.loc[max_index,hruName_field]
        pbar.update(1)
    pbar.close()

    in_gpd_disv = in_gpd_disv.dissolve(by=hruName_field)
    in_gpd_disv = in_gpd_disv.reset_index() 
    in_gpd_disv[hruArea_field] = in_gpd_disv.area       

    # PART 2. update HRUId and HRUNo based on elimination result
    # identify the max number of HRUs among all grus and its integer length
    max_hru_num = [len(in_gpd_disv[in_gpd_disv[gruNo_field]==gru]) for gru in grus]
    max_hru_str_len = max([len(str(max(max_hru_num))),2]) # str_len min is 2 (e.g, 01,02,...) 
    # loop through HRU to update HRUNo and HRUId
    for irow, row in in_gpd_disv.iterrows():
        target_gru = in_gpd_disv.loc[irow,gruNo_field]
        cum_df = in_gpd_disv[0:irow+1]
        hru_num = len(cum_df[cum_df[gruNo_field]==target_gru])
        hru_num_str = str(hru_num).zfill(max_hru_str_len)
        # update two fields        
        in_gpd_disv.at[irow,hruNo_field]=irow+1    
        in_gpd_disv.at[irow,hruName_field]=int(str(in_gpd_disv.loc[irow,gruName_field])+hru_num_str)

    # change dtypes of No, Name and class fields to be int64
    for field in [hruNo_field,gruNo_field,hruName_field,gruName_field]:   
        in_gpd_disv[field] = pd.to_numeric(in_gpd_disv[field], errors='coerce')
    for field in fieldname_list:   
        in_gpd_disv[field] = pd.to_numeric(in_gpd_disv[field], errors='coerce')

    # drop index column
    if 'index' in in_gpd_disv.columns:
        in_gpd_disv = in_gpd_disv.drop(columns=['index'])
    in_gpd_disv.to_file(hru_vector_disv, index=False)

    # PART 3. convert hru_vector_disv to raster for zonal statistics
    rasterize_vector(hru_vector_disv,hruNo_field, hruNo_field_dtype,refraster,hru_raster_disv)
    return

# Eliminate small HRUs by merging with the largest neighbor HRU within the same GRU.
def eliminate_small_hrus_neighbor(hru_vector, hru_thld_type, hru_thld, gruNo_field, gruName_field, 
                                  hruNo_field, hruNo_field_dtype, hruName_field,hruArea_field, 
                                  fieldname_list, refraster, hru_vector_disv, hru_raster_disv):
    '''
    hru_vector:        input, HRU shapefile.
    hru_area_thld:     input, number, small HRU area threthold below which the HRU will be merged with another HRU.
    gruNo_field:       input, string, field name of the gru number column, e.g.,1,2,3... 
    gruName_field:     input, string, field name of gru name, e.g., 100800120101. 
    hruNo_field:       input, string, field name of the hru number column, e.g.,1,2,3...
    hruNo_field_dtype: input, string, data type of hruNo_field, used to save hru raster.
    hruName_field:     input, string, field name of the hru name column, e.g., 10080012010101, 100800120102. 
    hruArea_field:     input, string, field name of the HRU area, used in small HRU elimination.
    fieldname_list:    input, string list, a list of filed names used in hru generation.
    refraster:         input, raster, a reference raster used to rasterize outvector.
    outraster:         output, raster, HRU raster with integer value.
    outvector:         output, raster, HRU vector with HRU_full and HRU_int fields. '''

    print('    reading hru shapefile')
    in_gpd          = gpd.read_file(hru_vector)
    print('    buffering topology')
    in_gpd.geometry = in_gpd.geometry.buffer(0)   # fix topology errors
    in_gpd_disv     = in_gpd.copy()
    in_gpd_disv     = in_gpd_disv.to_crs(in_gpd.crs)  # convert projection   
    num_orig_hrus   = len(in_gpd_disv)
    
    print('    eliminating ....')

    # PART 1. eliminate small HRUs based on HRU area threshold
    no_neighbors_count = 0
    grus = np.unique(in_gpd_disv[gruNo_field].values)
    pbar = tqdm(total=len(grus))
    for gru in grus:
        flt1        = (in_gpd_disv[gruNo_field]==gru)          # filter 1: gru
        gru_df      = in_gpd_disv[flt1]                        # gru dataframe
        #print('processing GRU', gru, ' with', len(gru_df), 'hrus')    
        if sum(gru_df.is_valid == False) > 0:                  # check validity
            print('Topology validity problems found, may cause problems in further processing')
            print('STOP')
        dom_hru_idx = gru_df[hruArea_field].idxmax()           # the most dominant HRU index in gru_df
        dom_hru     = gru_df.loc[dom_hru_idx,hruName_field]    # the most dominant HRU hruName in gru_df
        
        # update hru_area_thld if hru_thld_type is a fraction
        if hru_thld_type == 'fraction':
            gru_area      = gru_df[hruArea_field].sum()
            hru_area_thld = hru_thld*gru_area
        elif hru_thld_type == 'value':
            hru_area_thld = hru_thld        
        flt2    = (in_gpd_disv[hruArea_field] <= hru_area_thld)  # filter 2: HRU area    
        elim_df = in_gpd_disv[flt1 & flt2]                     # to-be eliminated HRU dataframe    

        row_num  = len(gru_df)                                 # total number of HRUs within the gru
        elim_num = len(elim_df)                                # number of to-be eliminated HRUs
        iter_num = 0                                           # elimination iteration number 
        #print('number of HRUs to eliminate:', elim_num)
        
        # elimination
        while row_num>1 and elim_num != 0 and iter_num < 10:
        # print(('GRU=%s, Iteration=%d, Target=%d/%d HRUs.')%(gru,iter_num+1,elim_num,row_num))

            for irow, row in elim_df.iterrows():
                # identify the target HRU's name and its neighbours' index in gru_df
                target_hruName = elim_df.loc[irow, hruName_field]
                nbhds_idx      = gru_df[gru_df.geometry.touches(row['geometry'])].index.tolist() 
                #print('elimating hru',irow)

                # update hruName_field depending on its neighbors
                if len(nbhds_idx)>0:
                    # when there are neightbors, take the attribute of the largest neighboring HRU.
                    larg_nbhd_idx = gru_df.loc[nbhds_idx,hruArea_field].idxmax()
                    for field in fieldname_list:
                        in_gpd_disv.at[in_gpd_disv[hruName_field]==target_hruName,field]     = gru_df.loc[larg_nbhd_idx,field]
                    in_gpd_disv.at[in_gpd_disv[hruName_field]==target_hruName,hruName_field] = gru_df.loc[larg_nbhd_idx,hruName_field] 
                else:
                    # when there is no neighbor, take the attribute of the most dominant HRU within the gru. 
                    no_neighbors_count = no_neighbors_count + 1
                    #print('      warning: no neighbors, using dominant gru type instead')  # print statements mess up progress bar
                    for field in fieldname_list:
                        in_gpd_disv.at[in_gpd_disv[hruName_field]==target_hruName,field]     = gru_df.loc[dom_hru_idx,field]
                    in_gpd_disv.at[in_gpd_disv[hruName_field]==target_hruName,hruName_field] = dom_hru 

            # dissolve in_gpd_disv based on hruName_field column and change hruName_field from index to column
            #in_gpd_disv['geometry'] = in_gpd_disv.geometry.buffer(0)   # buffer (add)
            in_gpd_disv = in_gpd_disv.dissolve(by=hruName_field)
            in_gpd_disv = in_gpd_disv.reset_index() 

            # update HRU area and filters
            in_gpd_disv[hruArea_field] = in_gpd_disv.area       
            flt1        = (in_gpd_disv[gruNo_field]==gru) 
            flt2        = (in_gpd_disv[hruArea_field]<=hru_area_thld)
            gru_df      = in_gpd_disv[flt1]
            elim_df     = in_gpd_disv[flt1 & flt2] 
            dom_hru_idx = gru_df[hruArea_field].idxmax()   
            dom_hru     = gru_df.loc[dom_hru_idx,hruName_field] 

            # udpdate counts
            row_num  = len(gru_df) 
            elim_num = len(elim_df) 
            iter_num = iter_num+1 
        pbar.update(1)
    pbar.close()
    print('      -- info: no neighbors found for %d hrus, used dominant gru type instead' % no_neighbors_count)

    # PART 2. update HRUId and HRUNo based on elimination result
    print('    updating hruIds, hru numbers and datatypes')
    # identify the max number of HRUs among all grus and its integer length
    max_hru_num     = [len(in_gpd_disv[in_gpd_disv[gruNo_field]==gru]) for gru in grus]
    max_hru_str_len = max([len(str(max(max_hru_num))),2]) # str_len min is 2 (e.g, 01,02,...) 
    # loop through HRU to update HRUNo and HRUId
    for irow, row in in_gpd_disv.iterrows():
        target_gru  = in_gpd_disv.loc[irow,gruNo_field]
        cum_df      = in_gpd_disv[0:irow+1]
        hru_num     = len(cum_df[cum_df[gruNo_field]==target_gru])
        hru_num_str = str(hru_num).zfill(max_hru_str_len)        
        # update two fields
        in_gpd_disv.at[irow,hruNo_field]   = irow+1    
        in_gpd_disv.at[irow,hruName_field] = int(str(in_gpd_disv.loc[irow,gruName_field])+hru_num_str)

    # change dtypes of No, Name and class fields to be int64
    for field in [hruNo_field,gruNo_field,hruName_field,gruName_field]:   
        in_gpd_disv[field] = pd.to_numeric(in_gpd_disv[field], errors='coerce')
    for field in fieldname_list:   
        in_gpd_disv[field] = pd.to_numeric(in_gpd_disv[field], errors='coerce')

    if 'index' in in_gpd_disv.columns:
        in_gpd_disv = in_gpd_disv.drop(columns=['index'])
    in_gpd_disv.to_file(hru_vector_disv, index=False)
    
    print('    done: eliminated %d hrus out of original %d' % (num_orig_hrus-len(in_gpd_disv), num_orig_hrus))

    # PART 3. convert hru_vector_disv to raster for zonal statistics
    rasterize_vector(hru_vector_disv,hruNo_field, hruNo_field_dtype,refraster,hru_raster_disv)
    return

# Calculate zonal statistics for a given input vector and raster attribute.
def zonal_statistic(attr_raster, invector, infield, infield_dtype, refraster, metric, out_raster, *args, **kwargs):
    '''
    attr_raster: input, raster. Raster to analyze.
    invector: input, vector. Vector with zones boundaries.
    infield: input, string. Field name of invector that is used to identify zone of invector.
    infield_dtype: input, string. data type of infield, used to rasterize invector.
    metric: input, string. metric to calcualte zonal statistics. Choose from 'mean', 'median', 'max', 'min'.
    out_raster: output, raster. Raster to save the zonal attribtue value of invector. 
    raster_band: optional input, integer. Number of raster band to analyze.
    output_column_prefix: optional input, str. Prefix for output fields.'''

    output_column_prefix = kwargs.get('output_column_prefix', 'DN')
    raster_band = kwargs.get('raster_band', 1)
        
    # --- PART 1. rasterize invector based on the relative resolution of attr_raster ---
    # read attr_raster and refraster resolutions
    with rio.open(attr_raster) as ff:
        attr_SizeX, attr_SizeY = ff.res
    with rio.open(refraster) as ff:
        ref_SizeX, ref_SizeY   = ff.res
        
    # choose the raster that has a smaller resolution from attr_size and refraster.
    # This process is to avoid converting invector into a resolution that is larger than refraster.
    # Converting invector to a larger resolution will cause no-data metric value in certain grids of in_raster.
    if attr_SizeX <= ref_SizeX:
        refraster = attr_raster
        # no need to resample attribtue raster
    else:
        refraster = refraster 
        # need to resample attribute raster
        attr_raster_resample = os.path.join(os.path.dirname(attr_raster), os.path.basename(attr_raster).split('.')[0]+'_resample.tif')
        resample_raster(attr_raster,refraster,attr_raster_resample)
        # change attr_raster_resample as the new attr_raster for the following calculation
        attr_raster = attr_raster_resample
    # convert invector into the chosen refraster resolution
    in_raster = os.path.join(os.path.dirname(invector), os.path.basename(invector).split('.')[0]+'tmp.shp')    
    rasterize_vector(invector,infield,infield_dtype,refraster,in_raster)

    # --- PART 2. loop through HRU to get attribute value ---
    # read invector
    in_gpd = gpd.read_file(invector)
    with rio.open(in_raster) as ff:
        in_raster_value = ff.read(1)
        in_raster_mask  = ff.read_masks(1)
    # read new attr_raster 
    with rio.open(attr_raster) as ff:
        attr_raster_value = ff.read(raster_band)
        out_meta          = ff.meta.copy()

    # initialize the output column prefix 
    if output_column_prefix in in_gpd.columns:
        in_gpd = in_gpd.drop(columns=[output_column_prefix])
    in_gpd[output_column_prefix] = "" 

    # initialize the output array
    output_array = attr_raster_value.copy()
    
    # loop through invector polygons
    unique_field_value = np.unique(in_raster_value[in_raster_mask!=0])
    for field_value in unique_field_value:
        smask = in_raster_value == field_value         
        attr_smask = attr_raster_value[smask]
        if isinstance(metric, str):
            if metric == 'mean':
                zonal_value = np.nanmean(attr_smask)
            elif metric == 'mean_aspect':    
                # reference: https://www.reddit.com/r/gis/comments/hulxk2/calculating_mean_aspect_for_a_polygon/
                # reference: https://geo.libretexts.org/Courses/University_of_California_Davis/UCD_GEL_56_-_Introduction_to_Geophysics/Geophysics_is_everywhere_in_geology.../zz%3A_Back_Matter/Arctan_vs_Arctan2
                # Method: decompose the aspect angle into a 2D vector using sin and cos, average the vectors, and then recompose them into an angle with arctan2.
                # Note: arctan is the 2-quadrant inverse tangent, it takes only one input value x/y. Its result is in [-90,90].
                # Note: arctan2 is the 4-quadrant inverse tangent, it takes two input arguments x and y. Its result is in [-180,180].
                attr_smask = np.where(attr_smask==0.0,np.nan,attr_smask) # exclude flat grids (flat aspect=0)
                
                y = np.sin(np.radians(attr_smask)) # north/south vector (positive to N)
                x = np.cos(np.radians(attr_smask)) # west/east vector (positive to E)
                zonal_y = np.nanmean(y)
                zonal_x = np.nanmean(x)
                zonal_value = np.degrees(np.arctan2(zonal_y, zonal_x))
                if zonal_value<0:
                    zonal_value = 360.0+zonal_value
            elif metric == 'mode':
                zonal_value = stats.mode(attr_smask,nan_policy='omit')[0]

            in_gpd.loc[in_gpd[infield]==field_value, output_column_prefix] = zonal_value
            output_array[smask] = zonal_value
        else: 
            print("Invalid metric string. Please choose from 'mean', 'median', 'max', 'min'.")
    
    # change astype as the same as attr_raster dtype
    in_gpd[output_column_prefix] = in_gpd[output_column_prefix].astype(out_meta['dtype'])
    output_array                 = output_array.astype(out_meta['dtype'])
    
    # save vector
    in_gpd.to_file(invector)
    os.remove(in_raster)

    # save outraster
    with rio.open(out_raster, 'w', **out_meta) as outf:
        out_arr_ma = np.ma.masked_array(output_array, in_raster_mask==0)
        outf.write(out_arr_ma,1)             
    return

def merge_shp_spatial_join(src1, src2,dst,merge_attr):
    """Perform spatial join on shp file data.
        Read in and output to new file
        """

    if os.path.exists(dst):
        print('Full-domain merged shapefile already exists, not overwriting')
        return
    
    src1_gdf = gpd.read_file(src1)
    src2_gdf = gpd.read_file(src2)
    
    # Check if the GeoDataFrames have the same projection
    if src1_gdf.crs != src2_gdf.crs:
        raise ValueError("GeoDataFrames have different projections.")
    
    src2_gdf = src2_gdf.drop(columns='geometry')
    src2_df = pd.DataFrame(src2_gdf)

    src1_gdf = src1_gdf.merge(src2_df, on=merge_attr)

    src1_gdf.to_file(dst)

    return

def read_raster(file):
    with rio.open(file) as ff:
        data = ff.read(1)
        mask = ff.read_masks(1)
    data_ma = np.ma.masked_array(data, mask==0)
    return data_ma

# not sure the following works; not used
def plot_vector(invector, column):    
    in_gpd  = gpd.read_file(invector)
    fig, ax = plt.subplots(figsize=(10, 6))
    in_gpd.plot(column=column, ax=ax)
    ax.set_axis_off()
    # plt.axis('equal')
    plt.show()   
    return

def crop_raster_with_buffer(shapefile_path, raster_path, output_path, buffer_distance):
    # Load the shapefile using geopandas
    shapefile = gpd.read_file(shapefile_path)
    
    # Calculate the bounding box of the shapefile
    minx, miny, maxx, maxy = shapefile.total_bounds
    
    # Create a buffered bounding box
    bbox = box(minx - buffer_distance, miny - buffer_distance, 
               maxx + buffer_distance, maxy + buffer_distance)
    
    # Create a GeoDataFrame from the buffered bounding box
    geo = gpd.GeoDataFrame({'geometry': bbox}, index=[0], crs=shapefile.crs)
    
    # Open the raster file
    with rasterio.open(raster_path) as src:
        # Clip the raster with the buffered bounding box using rasterio's mask function
        out_image, out_transform = mask(src, geo.geometry, crop=True)
        out_meta = src.meta.copy()
        
        # Update the metadata with new dimensions, transform, and CRS
        out_meta.update({
            "driver": "GTiff",
            "height": out_image.shape[1],
            "width": out_image.shape[2],
            "transform": out_transform
        })
        
        # Write the clipped raster to an output file
        with rasterio.open(output_path, "w", **out_meta) as dest:
            dest.write(out_image)